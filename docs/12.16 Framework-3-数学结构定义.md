我们将基于 **概率图模型 (Probabilistic Graphical Models)**、**信息论 (Information Theory)** 和 **自由能原理 (Free Energy Principle)** 来构建整套数学结构。

以下是 PCM 框架的完整数学形式化定义。

---

# Mathematical Formalization of Proactive Cognitive Memory (PCM)

## 1. 基础符号定义 (Preliminaries)

我们定义 $t$ 时刻的系统状态。

*   **User Input:** $u_t$ (用户当前的自然语言 Query)。
*   **System Response:** $r_t$ (Agent 生成的回复)。
*   **Working Memory (L1):** $Q_t = [u_{t-k}, r_{t-k}, \dots, u_t]$ (滑动窗口内的对话历史)。
*   **World Model (L2):** $G_t = (\mathcal{V}_t, \mathcal{E}_t)$ (概率知识图谱)。

---

## 2. Layer 2: 概率世界模型 (The Probabilistic World Model)

我们将 Layer 2 形式化为一个 **动态加权有向图 (Dynamic Weighted Directed Graph)**。

### 2.1 图谱定义
$$ G_t = (\mathcal{V}_t, \mathcal{E}_t) $$

*   **节点集合 $\mathcal{V}_t$:** 包含实体 $e$、属性 $a$ 和 **假设 $h$**。
*   **边集合 $\mathcal{E}_t$:** 每一条边 $k$ 定义为一个五元组：
    $$ \epsilon_k = (v_i, v_j, \text{rel}, w_k, \tau_k) $$
    *   $v_i, v_j \in \mathcal{V}_t$: 头尾节点。
    *   $\text{rel}$: 语义关系类型。
    *   **$w_k \in [0, 1]$: 置信度权重 (Confidence Weight)**。表示该知识点为“真”的后验概率 $P(\text{True} | \text{History})$。
    *   $\tau_k$: 最后更新时间戳 (Last Updated Timestamp)。

### 2.2 权重的语义 (Semantics of Confidence)
权重的初始化与更新遵循 **贝叶斯置信度 (Bayesian Confidence)** 逻辑。
*   $w \approx 1.0$: 确凿事实 (Fact)。
*   $w \approx 0.5$: 待验证的假设 (Hypothesis)。
*   $w \approx 0.1$: 已被证伪或过时的记忆 (Falsified/Outdated)，但在图中保留以备回溯（Soft Delete）。

---

## 3. Layer 1: 感知与意图掩码检索 (Perception & Intent-Masked Retrieval)

为了处理检索噪声，我们引入 **意图分布 (Intent Distribution)** 作为检索的先验掩码。

### 3.1 意图路由 (Intent Routing)
设预定义的意图领域集合为 $\mathcal{D} = \{d_1, d_2, \dots, d_m\}$ (e.g., Coding, Academic, Personal)。
Intent Router 是一个分类模型 $f_{\theta}$，输出当前 Query $u_t$ 在各领域上的概率分布：

$$ P(I_t | u_t, Q_t) = \text{Softmax}(f_{\theta}(u_t, Q_t)) $$

其中 $I_t \in \mathbb{R}^m$ 是意图向量，$\sum I_t[j] = 1$。

### 3.2 考虑噪声的检索评分 (Retrieval Scoring with Noise)
传统的检索仅依赖余弦相似度 $\text{sim}(u_t, \epsilon_k)$。为了抑制噪声，我们引入 **意图掩码 (Intent Mask)** 和 **置信度门控 (Confidence Gating)**。

对于图中的每一条边 $\epsilon_k$，其检索相关性得分 $Score(\epsilon_k)$ 定义为：

$$ Score(\epsilon_k) = \underbrace{\text{sim}(\text{emb}(u_t), \text{emb}(\epsilon_k))}_{\text{Semantic Similarity}} \cdot \underbrace{P(d(\epsilon_k) | u_t)}_{\text{Intent Relevance}} \cdot \underbrace{w_k}_{\text{Confidence}} $$

*   $d(\epsilon_k)$: 边 $\epsilon_k$ 所属的领域。
*   **抗噪机制：** 即使语义相似度高（例如“Python”字面匹配），如果意图不符（用户在问生物学蟒蛇）或置信度低（已过时），最终 Score 也会被压制。

最终检索到的上下文 $C_t$ 为 Top-K 边的集合：
$$ C_t = \{ \epsilon_k \mid \epsilon_k \in \mathcal{E}_t, Score(\epsilon_k) > \delta \} $$

---

## 4. 惊奇度计算 (Surprisal Calculation) —— *The Theoretical Core*

这是连接 L1 和 L3 的核心指标。我们基于 **自由能原理 (Friston, 2010)** 和 **信息论 (Shannon, 1948)**。

### 4.1 基础惊奇度 (Raw Surprisal)
定义为用户输入 $u_t$ 在当前世界模型 $G_t$（通过检索上下文 $C_t$ 近似）下的 **负对数似然 (Negative Log-Likelihood, NLL)**：

$$ S_{raw}(u_t) = - \log P_{\text{LLM}}(u_t | C_t, Q_{t-1}) $$

### 4.2 有效惊奇度 (Effective Surprisal) —— *考虑检索不确定性*
**创新点：** 如果检索本身充满了噪声（即我们根本没找到相关记忆），那么 NLL 高并不代表“冲突”，只代表“无知”。我们需要区分 **认知不确定性 (Epistemic Uncertainty)** 和 **偶然不确定性 (Aleatoric Uncertainty)**。

我们引入 **检索熵 (Retrieval Entropy)** $H(C_t)$ 来衡量检索质量：
$$ H(C_t) = - \sum_{\epsilon \in C_t} \hat{s}(\epsilon) \log \hat{s}(\epsilon) $$
其中 $\hat{s}$ 是归一化的检索得分。

**有效惊奇度 $S_{eff}$ 定义为：**

$$ S_{eff}(u_t) = S_{raw}(u_t) \cdot \underbrace{(1 - \lambda \cdot H(C_t))}_{\text{Confidence Gate}} $$

*   **解释：**
    *   如果检索结果很明确（$H(C_t)$ 低），我们信任 $S_{raw}$，认为这是真正的冲突。
    *   如果检索结果很混乱（$H(C_t)$ 高），说明系统没找到相关知识，此时降低惊奇度权重，避免 L3 产生错误的“修正”幻觉（可能只是没记过）。

---

## 5. Layer 3: 认知演化 (Cognitive Evolution)

基于 $S_{eff}$，我们将演化过程形式化为三种不同的 **贝叶斯更新策略**。

### 5.1 贝叶斯权重更新通用公式
根据贝叶斯定理，后验置信度 $\propto$ 似然 $\times$ 先验。
$$ w_{t+1} = \frac{P(u_t | \epsilon) \cdot w_t}{P(u_t)} $$

在我们的框架中，我们将 $P(u_t | \epsilon)$ 近似为与惊奇度相关的函数。

### 5.2 分级演化策略 (Tiered Evolution Strategies)

#### Case A: Low Surprise ($S_{eff} \le \theta_{low}$) -> Maintenance (维护)
*   **场景：** 观测符合预期。
*   **数学操作 (Reinforcement):** 对 $C_t$ 中的边进行渐进式增强。
    $$ w_{t+1} = w_t + \eta \cdot (1 - w_t) $$
    *   $\eta$: 学习率 (e.g., 0.05)。这使得权重无限趋近于 1 但永不溢出。
    *   $\tau_{t+1} = t$ (更新时间戳)。

#### Case B: Medium Surprise ($\theta_{low} < S_{eff} \le \theta_{high}$) -> Profiling (侧写/假设)
*   **场景：** 观测偏离预期，但未构成直接逻辑矛盾。
*   **数学操作 (Hypothesis Generation):**
    1.  **Node Creation:** 创建新节点 $h_{new}$ (Hypothesis)。
    2.  **Initialization:** 赋予初始权重 $w_{init}$。
        $$ w_{init} = \sigma(S_{eff}) \quad (\text{Sigmoid function mapped to } [0.3, 0.5]) $$
        *   惊奇度越高（在阈值范围内），说明该假设的信息量越大，初始关注度越高。
    3.  **Implicit Verification (隐式验证):**
        在未来的时间步 $t+k$，如果再次检索到 $h_{new}$ 且 $S_{eff}$ 变低（说明假设解释了新数据），则按 Case A 增强权重；否则按时间衰减：
        $$ w_{t+k} = w_t \cdot e^{-\gamma \Delta t} \quad (\text{Time Decay}) $$

#### Case C: High Surprise ($S_{eff} > \theta_{high}$) -> Correction (修正)
*   **场景：** 观测与现有高权重记忆 $C_t$ 发生剧烈冲突。
*   **数学操作 (Conflict Resolution):**
    1.  **Penalty (惩罚):** 对冲突的旧边 $\epsilon_{old}$ 进行指数衰减。
        $$ w_{t+1} = w_t \cdot \exp(-\beta \cdot S_{eff}) $$
        *   惊奇度 $S_{eff}$ 越大，惩罚力度 $\beta$ 越强。
    2.  **Overwrite (覆盖):** 创建新边 $\epsilon_{new}$，并赋予高权重。

---

## 6. 总结：数学逻辑闭环

这套形式化定义实现了以下逻辑闭环：

1.  **L1 检索公式** 考虑了 **意图 ($P(I)$)** 和 **权重 ($w$)**，并引入了 **噪声控制**。
2.  **惊奇度公式** 引入了 **检索熵 ($H(C)$)**，区分了“无知”和“冲突”。
3.  **L3 演化公式** 统一在 **贝叶斯更新** 的框架下，分别对应权重的 **增强 (Reinforcement)**、**初始化 (Initialization)** 和 **衰减 (Decay)**。