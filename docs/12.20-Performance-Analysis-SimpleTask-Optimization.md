# ProCoMemory Performance Analysis: Simple Task Optimization

**Date:** 2024-12-20
**Author:** Analysis based on comparative study with O-mem
**Version:** 1.0

---

## Executive Summary

ProCoMemory demonstrates strong performance on Temporal (F1=0.393) and Adversarial tasks but significantly underperforms on simple retrieval tasks like Single-hop (F1=0.138), Multi-hop (F1=0.254), and Open-domain (F1=0.222) questions. This document analyzes the root causes and proposes optimization strategies.

---

## 1. Current Performance Metrics

### 1.1 LoCoMo Benchmark Results (conv-26 sample)

| Category | Count | Exact Match | F1 Score | Contains | Error Rate |
|----------|-------|-------------|----------|----------|------------|
| Single-hop | 32 | 0.000 | 0.138 | 0.094 | **78.1%** |
| Temporal | 37 | 0.162 | 0.393 | 0.324 | 51.4% |
| Multi-hop | 13 | 0.000 | 0.254 | 0.538 | 69.2% |
| Adversarial | 70 | 0.100 | 0.292 | 0.157 | 64.3% |
| Open-domain | 47 | 0.128 | 0.222 | 0.128 | N/A |
| **Overall** | 199 | 0.096 | 0.267 | 0.196 | 68.3% |

### 1.2 Key Observations

1. **Single-hop tasks have the highest error rate (78.1%)** - These are simple factual lookups that should be easiest
2. **Temporal tasks perform best** - Surprisal-based reasoning helps with time-related queries
3. **Common failure pattern**: "The memory context does not provide information about..." (false negatives)

---

## 2. Root Cause Analysis

### 2.1 Problem 1: No Structured Fact Extraction

**Current Behavior:**
ProCoMemory stores raw dialogue messages as FACT nodes:
```json
{
  "content": "[1:56 pm on 8 May, 2023] Melanie: Yeah, I painted that lake sunrise last year! It's special to me.",
  "node_type": "fact",
  "domain": "Personal",
  "weight": 0.743
}
```

**What's Missing:**
- No extraction of atomic facts: `Melanie → painted → sunrise`, `time → last year (2022)`
- No entity extraction: `Melanie`, `lake`, `painting`
- No attribute extraction: `Melanie.hobby = painting`

**O-mem's Approach:**
```python
# O-mem extracts structured data from each message:
{
    "raw_message": "Yeah, I painted that lake sunrise last year!",
    "message": "Melanie painted a sunrise",  # Summarized
    "topics": "painting, art, hobbies",
    "emotions": "Positive",
    "fact": "Melanie painted a lake sunrise last year",
    "attribute": ["hobby: painting", "activity: art"]
}
```

**Impact:**
- Question: "When did Melanie paint a sunrise?"
- Expected: "2022"
- ProCoMemory returns: "Last year." (lacks temporal grounding)
- Root cause: No fact normalization to extract "2022" from "last year" relative to message timestamp

---

### 2.2 Problem 2: Embedding-Only Retrieval (No Keyword Matching)

**Current Retrieval Formula** (layer2_world_model.py):
```python
Score(ε_k) = sim(emb(u_t), emb(ε_k)) × P(d(ε_k) | u_t) × w_k
```
Where:
- `sim()` = Cosine similarity (semantic only)
- `P(d | u)` = Intent relevance probability
- `w_k` = Node weight

**What's Missing:**
- No keyword/lexical matching (TF-IDF, BM25)
- No inverted index for exact term lookup
- Rare terms may not surface in semantic search

**O-mem's Multi-Modal Retrieval:**
```python
# Step 1: Embedding similarity
embedding_scores = cosine_similarity(query_emb, fact_embeddings)

# Step 2: TF-IDF keyword matching
keyword_scores = tfidf_match(query, inverted_index)

# Step 3: Soft segmentation with drop threshold
activated_facts = activate_fact_memory_match(scores, drop_threshold=0.1)

# Combined score
final_score = α * embedding_scores + β * keyword_scores
```

**Impact Example:**
- Question: "Where did Caroline move from 4 years ago?"
- Answer: "Sweden"
- "Sweden" appears exactly once in 582 nodes
- Semantic similarity may not rank this highly, but TF-IDF would catch the rare keyword

---

### 2.3 Problem 3: Knowledge Graph Structure Unused (0 Edges)

**Current State** (from knowledge_graph.json):
```json
{
  "metadata": {
    "num_nodes": 582,
    "num_edges": 0  // No relationships!
  }
}
```

**Defined but Not Used:**
- `MemoryEdge` class with relations: "supersedes", "derived_from", "related"
- `retrieve_with_expansion()` method exists but `expansion_depth=1` has no effect without edges
- Graph neighbor expansion is a no-op

**What Should Happen:**
```
[Caroline moved from Sweden] --related--> [Caroline has Swedish grandma]
                            --temporal--> [4 years ago = 2019]
[Melanie plays violin] --related--> [Melanie plays clarinet]
                      --derived_from--> [Melanie does self-care activities]
```

---

### 2.4 Problem 4: Insufficient Retrieval Context

**Comparison of Retrieved Context:**

| Question | O-mem Retrieved | ProCoMemory Retrieved |
|----------|-----------------|----------------------|
| "When did Caroline go to LGBTQ support group?" | 22 relevant messages | 3 messages |
| "What instruments does Melanie play?" | Facts + raw messages | 0 relevant (wrong retrieval) |

**Analysis of ProCoMemory Retrieval Failures:**

```
Question: "What instruments does Melanie play?"
Expected: "clarinet and violin"
Retrieved Context:
  1. "[3:19 pm on 28 August, 2023] Melanie: Cool! What type of music do you play?"
  2. "[3:19 pm on 28 August, 2023] Caroline: Thanks, Melanie! Appreciate it. You play any instruments?"
  3. "[3:19 pm on 28 August, 2023] Melanie: That's awesome! What type of guitar? Been playing long?"

Problem: Retrieved context discusses instruments but doesn't contain Melanie's actual answer about clarinet/violin
```

The retrieval is semantically relevant (about instruments) but misses the specific message containing the answer.

---

### 2.5 Problem 5: No Persona/Summary Memory Layer

**O-mem's Three-Tier Architecture:**
```
┌─────────────────────────────────────────────────────────────┐
│                     Persona Memory                          │
│  ┌─────────────────┬─────────────────┬─────────────────┐   │
│  │ preference_     │ fact_persona    │ attr_persona    │   │
│  │ persona         │                 │                 │   │
│  │ "likes outdoor  │ "has 3 kids"    │ "from Sweden"   │   │
│  │  activities"    │ "plays violin"  │ "age 28"        │   │
│  └─────────────────┴─────────────────┴─────────────────┘   │
└─────────────────────────────────────────────────────────────┘
                              ↑ consolidation
┌─────────────────────────────────────────────────────────────┐
│                    Episodic Memory                          │
│  Events, Topics, Facts with timestamps and round citations  │
└─────────────────────────────────────────────────────────────┘
                              ↑ overflow
┌─────────────────────────────────────────────────────────────┐
│                    Working Memory                           │
│  Recent messages in FIFO queue (token-limited)              │
└─────────────────────────────────────────────────────────────┘
```

**ProCoMemory's Structure:**
```
┌─────────────────────────────────────────────────────────────┐
│                  WeightedKnowledgeGraph (L2)                │
│  - All messages stored as FACT nodes                        │
│  - No hierarchy or consolidation                            │
│  - No persona extraction                                    │
└─────────────────────────────────────────────────────────────┘
                              ↑
┌─────────────────────────────────────────────────────────────┐
│                SlidingContextQueue (L1)                     │
│  - Token-limited working memory                             │
│  - Evicted data → L3 for processing                         │
└─────────────────────────────────────────────────────────────┘
```

**Missing:** A consolidated persona layer for quick lookup of user attributes.

---

## 3. Proposed Optimization Strategies

### 3.1 Strategy 1: Structured Fact Extraction Pipeline

**Implementation Location:** `layers/layer1_perception.py` or new `utils/fact_extractor.py`

**Approach:**
```python
class StructuredFactExtractor:
    """Extract atomic facts from dialogue messages."""

    def extract(self, message: str, timestamp: str) -> List[StructuredFact]:
        """
        Input: "[1:56 pm on 8 May, 2023] Melanie: Yeah, I painted that lake sunrise last year!"

        Output: [
            StructuredFact(
                subject="Melanie",
                predicate="painted",
                object="lake sunrise",
                temporal="2022",  # Resolved from "last year" + message timestamp
                raw_message=message,
                confidence=0.9
            )
        ]
        """
        # Use LLM to extract structured facts
        prompt = FACT_EXTRACTION_PROMPT.format(message=message, timestamp=timestamp)
        facts = self.llm_client.extract_structured(prompt, StructuredFact)
        return facts
```

**Data Structure:**
```python
@dataclass
class StructuredFact:
    subject: str           # Entity (e.g., "Melanie")
    predicate: str         # Relation (e.g., "painted")
    object: str            # Object (e.g., "sunrise")
    temporal: Optional[str] # Resolved time (e.g., "2022")
    location: Optional[str] # Location if mentioned
    raw_message: str       # Original message
    source_timestamp: str  # When message was received
    confidence: float      # Extraction confidence
```

**Integration:**
- Extract facts during L1 processing
- Store both raw message AND structured facts in L2
- Create edges between related facts

---

### 3.2 Strategy 2: Hybrid Retrieval with Keyword Matching

**Implementation Location:** `layers/layer2_world_model.py`

**New Retrieval Formula:**
```python
Score(ε_k) = α × semantic_sim + β × keyword_sim + γ × graph_sim + δ × recency
```

Where:
- `α = 0.4` - Semantic embedding similarity
- `β = 0.3` - BM25/TF-IDF keyword score
- `γ = 0.2` - Graph neighbor relevance
- `δ = 0.1` - Temporal recency bonus

**Implementation:**
```python
class HybridRetriever:
    def __init__(self):
        self.embedding_index = VectorStore()
        self.keyword_index = InvertedIndex()  # NEW
        self.graph = KnowledgeGraph()

    def build_keyword_index(self, nodes: List[MemoryNode]):
        """Build inverted index for keyword lookup."""
        for node in nodes:
            tokens = self.tokenize(node.content)
            for token in tokens:
                if token not in STOP_WORDS:
                    self.keyword_index.add(token.lower(), node.id)

    def retrieve(self, query: str, top_k: int = 10) -> List[Tuple[MemoryNode, float]]:
        # Semantic retrieval
        semantic_results = self.embedding_index.search(query, top_k * 2)

        # Keyword retrieval (NEW)
        query_tokens = self.tokenize(query)
        keyword_results = self.keyword_index.search(query_tokens, top_k * 2)

        # Graph expansion
        expanded_nodes = self.expand_graph_neighbors(
            [r.id for r in semantic_results + keyword_results]
        )

        # Combine and re-rank
        combined = self.fusion_rerank(
            semantic_results,
            keyword_results,
            expanded_nodes,
            weights=[0.4, 0.3, 0.2, 0.1]
        )

        return combined[:top_k]
```

**Inverted Index Structure:**
```python
class InvertedIndex:
    def __init__(self):
        self.index: Dict[str, List[str]] = defaultdict(list)  # token -> [node_ids]
        self.doc_freq: Dict[str, int] = defaultdict(int)       # token -> doc count
        self.doc_lengths: Dict[str, int] = {}                   # node_id -> length

    def add(self, token: str, node_id: str):
        self.index[token].append(node_id)
        self.doc_freq[token] += 1

    def bm25_score(self, query_tokens: List[str], node_id: str) -> float:
        """Calculate BM25 score for a node given query tokens."""
        k1, b = 1.2, 0.75
        avg_dl = sum(self.doc_lengths.values()) / len(self.doc_lengths)

        score = 0.0
        for token in query_tokens:
            if node_id in self.index[token]:
                tf = self.index[token].count(node_id)
                idf = math.log((len(self.doc_lengths) - self.doc_freq[token] + 0.5) /
                              (self.doc_freq[token] + 0.5) + 1)
                dl = self.doc_lengths[node_id]
                score += idf * (tf * (k1 + 1)) / (tf + k1 * (1 - b + b * dl / avg_dl))

        return score
```

---

### 3.3 Strategy 3: Activate Knowledge Graph Edges

**Implementation Location:** `layers/layer3_evolution.py`

**Edge Creation Rules:**
```python
class EdgeCreator:
    def create_edges(self, new_fact: StructuredFact, existing_facts: List[StructuredFact]):
        edges = []

        # 1. Same subject → "related"
        for existing in existing_facts:
            if new_fact.subject == existing.subject:
                edges.append(MemoryEdge(
                    source_id=new_fact.id,
                    target_id=existing.id,
                    relation="related",
                    weight=0.7
                ))

        # 2. Contradictory facts → "supersedes"
        for existing in existing_facts:
            if self.is_contradictory(new_fact, existing):
                edges.append(MemoryEdge(
                    source_id=new_fact.id,
                    target_id=existing.id,
                    relation="supersedes",
                    weight=0.9
                ))
                # Decay the old fact's weight
                existing.weight *= 0.5

        # 3. Temporal sequence → "follows"
        for existing in existing_facts:
            if new_fact.subject == existing.subject:
                if parse_time(new_fact.temporal) > parse_time(existing.temporal):
                    edges.append(MemoryEdge(
                        source_id=new_fact.id,
                        target_id=existing.id,
                        relation="follows",
                        weight=0.8
                    ))

        return edges
```

**Graph Expansion During Retrieval:**
```python
def retrieve_with_expansion(self, query: str, top_k: int, expansion_depth: int = 1):
    # Initial retrieval
    initial_nodes = self.retrieve(query, top_k)

    if expansion_depth > 0 and self.graph.number_of_edges() > 0:  # Check edges exist!
        expanded = set()
        for node, score in initial_nodes:
            # Get 1-hop neighbors
            neighbors = self.graph.neighbors(node.id)
            for neighbor_id in neighbors:
                edge = self.graph.get_edge(node.id, neighbor_id)
                neighbor_node = self.get_node(neighbor_id)
                # Discount by edge weight
                expanded.add((neighbor_node, score * edge.weight * 0.5))

        # Merge and re-rank
        all_nodes = list(initial_nodes) + list(expanded)
        return sorted(all_nodes, key=lambda x: x[1], reverse=True)[:top_k]

    return initial_nodes
```

---

### 3.4 Strategy 4: Add Persona Memory Layer

**New Component:** `layers/persona_memory.py`

```python
@dataclass
class PersonaMemory:
    """Consolidated user profile for quick lookup."""

    user_id: str

    # Demographics/attributes
    attributes: Dict[str, str] = field(default_factory=dict)
    # e.g., {"hometown": "Sweden", "relationship_status": "Single", "occupation": "counselor"}

    # Preferences
    preferences: Dict[str, str] = field(default_factory=dict)
    # e.g., {"hobby": "painting, pottery", "music": "classical"}

    # Consolidated facts
    facts: List[str] = field(default_factory=list)
    # e.g., ["moved from Sweden 4 years ago", "has 3 children", "plays violin and clarinet"]

    # Source citations
    citations: Dict[str, List[str]] = field(default_factory=dict)
    # e.g., {"hometown": ["round 5", "round 12"]}


class PersonaMemoryManager:
    def consolidate(self, structured_facts: List[StructuredFact]) -> PersonaMemory:
        """Consolidate facts into persona attributes."""
        persona = PersonaMemory(user_id=self.user_id)

        for fact in structured_facts:
            # Attribute extraction
            if fact.predicate in ["is", "has", "lives_in", "from"]:
                key = self.normalize_attribute_key(fact.predicate, fact.object)
                persona.attributes[key] = fact.object

            # Preference extraction
            elif fact.predicate in ["likes", "enjoys", "prefers"]:
                key = self.normalize_preference_key(fact.object)
                if key in persona.preferences:
                    persona.preferences[key] += f", {fact.object}"
                else:
                    persona.preferences[key] = fact.object

            # General facts
            else:
                fact_str = f"{fact.subject} {fact.predicate} {fact.object}"
                if fact.temporal:
                    fact_str += f" ({fact.temporal})"
                persona.facts.append(fact_str)

        return persona

    def query_persona(self, query: str) -> Optional[str]:
        """Quick lookup for persona-related queries."""
        # Check if query matches attribute pattern
        for attr_key, attr_value in self.persona.attributes.items():
            if attr_key.lower() in query.lower():
                return attr_value

        # Check facts for exact keyword match
        query_tokens = set(query.lower().split())
        for fact in self.persona.facts:
            fact_tokens = set(fact.lower().split())
            if len(query_tokens & fact_tokens) >= 2:
                return fact

        return None
```

**Integration with Retrieval:**
```python
def retrieve(self, query: str, top_k: int):
    # First, check persona for quick lookup
    persona_result = self.persona_manager.query_persona(query)
    if persona_result:
        return [(persona_result, 1.0)]  # High confidence direct match

    # Fall back to full retrieval
    return self.hybrid_retriever.retrieve(query, top_k)
```

---

### 3.5 Strategy 5: Improved Response Generation with Context Verification

**Problem:** Even when correct context is retrieved, the LLM sometimes says "information not found"

**Solution:** Add a verification step:
```python
def generate_response(self, query: str, retrieved_context: List[str]) -> str:
    # Step 1: Verify context contains answer
    verification_prompt = f"""
    Query: {query}
    Context: {retrieved_context}

    Does the context contain information to answer the query?
    If yes, extract the specific information.
    If no, say "NOT_FOUND".
    """

    verification_result = self.llm.generate(verification_prompt)

    if "NOT_FOUND" in verification_result:
        # Try expanding retrieval
        expanded_context = self.retrieve_with_expansion(query, top_k=10, expansion_depth=2)
        return self.generate_response(query, expanded_context)

    # Step 2: Generate answer using verified context
    answer_prompt = f"""
    Query: {query}
    Relevant Information: {verification_result}

    Answer the query based on the information provided.
    Be specific and include dates/details when available.
    """

    return self.llm.generate(answer_prompt)
```

---

## 4. Implementation Priority

| Priority | Strategy | Effort | Expected Impact |
|----------|----------|--------|-----------------|
| **P0** | Hybrid Retrieval (keyword + semantic) | Medium | High - Fixes 30%+ of retrieval misses |
| **P1** | Structured Fact Extraction | High | High - Enables precise matching |
| **P1** | Activate Knowledge Graph Edges | Medium | Medium - Improves multi-hop |
| **P2** | Persona Memory Layer | Medium | Medium - Fast attribute lookup |
| **P3** | Response Verification | Low | Low - Reduces false negatives |

---

## 5. Metrics for Validation

After implementing optimizations, track:

| Metric | Current | Target |
|--------|---------|--------|
| Single-hop F1 | 0.138 | 0.50+ |
| Multi-hop F1 | 0.254 | 0.45+ |
| Temporal F1 | 0.393 | 0.50+ |
| Retrieval Recall@10 | ~30% | 70%+ |
| Knowledge Graph Edges | 0 | 1000+ |

---

## 6. Conclusion

ProCoMemory's current architecture excels at reasoning-heavy tasks (temporal, adversarial) due to its surprisal-based evolution mechanism. However, for simple factual retrieval, it lacks:

1. **Structured fact extraction** - Only stores raw messages
2. **Keyword-based retrieval** - Over-relies on semantic similarity
3. **Graph utilization** - 0 edges despite having graph infrastructure
4. **Persona consolidation** - No quick-lookup for user attributes

By implementing the proposed hybrid retrieval and fact extraction strategies, ProCoMemory can maintain its reasoning strengths while significantly improving simple task performance.

---

## Appendix A: Error Case Studies

### Case 1: Missing Instrument Information
```
Question: "What instruments does Melanie play?"
Expected: "clarinet and violin"
Actual: "The memory context does not specify any instruments that Melanie plays."

Analysis:
- The message "[1:14 pm on 25 May, 2023] Melanie: ...running, reading, or playing my violin..."
  exists in the knowledge graph
- Semantic search for "instruments Melanie play" returned messages about
  Caroline asking about instruments, not Melanie's answer
- With keyword search, "violin" would have been matched directly
```

### Case 2: Failed Location Retrieval
```
Question: "Where did Caroline move from 4 years ago?"
Expected: "Sweden"
Actual: "The memory context does not provide information..."

Analysis:
- The word "Sweden" appears exactly once in all 582 nodes
- Semantic similarity between "where moved from" and the Sweden message was low
- No persona attribute stored for Caroline.hometown = Sweden
```

### Case 3: Temporal Resolution Failure
```
Question: "When did Melanie paint a sunrise?"
Expected: "2022"
Actual: "Last year."

Analysis:
- Retrieved correct message: "I painted that lake sunrise last year!"
- Message timestamp: May 2023, so "last year" = 2022
- No temporal normalization applied during fact extraction
- LLM just echoed "last year" instead of calculating the year
```

---

## Appendix B: O-mem Key Components Reference

For implementation reference, key O-mem modules:

| Component | Location | Function |
|-----------|----------|----------|
| Fact Extraction | `memory_manager.py:60-88` | `UNDERSTAND_USER_EXPERIENCE_PROMPT` |
| Keyword Index | `memory.py:142-183` | `user_detail_dict` inverted index |
| Persona Memory | `persona_memory.py` | `preference_persona`, `fact_persona` |
| Hybrid Retrieval | `memory_manager.py:780-932` | `retrieve_from_memory_soft_segmentation` |
| Soft Segmentation | `memory_manager.py:538-561` | `activate_fact_memory_match` |
